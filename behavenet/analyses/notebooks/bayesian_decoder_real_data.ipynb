{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import pickle\n",
    "from ssm.stats import multivariate_normal_logpdf\n",
    "from ssm.primitives import hmm_expected_states, hmm_sample\n",
    "from pylds.lds_messages_interface import info_E_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = 8\n",
    "K = 16\n",
    "nlags = 3\n",
    "\n",
    "data = np.load('arhmm_decoding_data.npz')\n",
    "neural = data['neural']\n",
    "xs = data['latents']\n",
    "zs = data['states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to lag the neural activity for each batch\n",
    "\n",
    "W = 8\n",
    "N = neural[0].shape[1]\n",
    "\n",
    "ns_window = [_ for i in range(len(neural))]\n",
    "for i in range(len(neural)):\n",
    "    T = neural[i].shape[0]\n",
    "    neural_pad = np.concatenate([np.zeros((W//2, N)), neural[i], np.zeros((W//2, N))])\n",
    "    ns_window[i] = np.column_stack([neural_pad[w:T+w] for w in range(W)] )\n",
    "\n",
    "# Have to lag the latents for each batch \n",
    "W = nlags\n",
    "xs_eff = [_ for i in range(len(xs))]\n",
    "for i in range(len(neural)):\n",
    "    xs_pad = np.concatenate([np.zeros((W-1,D)), xs[i]])\n",
    "    xs_eff[i] = np.column_stack([xs_pad[w:T+w] for w in reversed(range(W))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatente into sequences, take only part for now\n",
    "T=3000\n",
    "ns_window_flat =np.concatenate(ns_window,axis=0)[:T]\n",
    "zs_flat = np.concatenate(zs,axis=0)[:T]\n",
    "xs_eff_flat = np.concatenate(xs_eff,axis=0)[:T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eleanorbatty/anaconda/envs/behavenet/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=500, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression\n",
    "\n",
    "recog_z = LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "recog_z.fit(ns_window_flat, zs_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear regression\n",
    "\n",
    "recog_x = LinearRegression()\n",
    "recog_x.fit(ns_window_flat, xs_eff_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in ARHMM model and get parameters\n",
    "\n",
    "arhmm_model = pickle.load(open('best_val_model.pt','rb'))\n",
    "\n",
    "P = arhmm_model.transitions.transition_matrix\n",
    "As = arhmm_model.observations.As\n",
    "bs = arhmm_model.observations.bs\n",
    "Qs = arhmm_model.observations.Sigmas\n",
    "\n",
    "evals, evecs = np.linalg.eig(P.T)\n",
    "perm = np.argsort(evals)[::-1]\n",
    "evals, evecs = evals[perm], evecs[:, perm]\n",
    "assert np.allclose(evals[0], 1.0)\n",
    "if np.any(evecs[:,0] <= 0):\n",
    "    evecs[:,0] = -1*evecs[:,0]\n",
    "assert np.all(evecs[:,0] >= 0) \n",
    "pz_infty = np.real(evecs[:, 0] / evecs[:, 0].sum())\n",
    "\n",
    "\n",
    "mu_infty = np.zeros((K, D*nlags))\n",
    "Sigma_infty = np.zeros((K, D*nlags, D*nlags))\n",
    "for k in range(K):\n",
    "    mu_infty[k] = np.mean(xs_eff_flat[zs_flat == k])\n",
    "    #Sigma_infty[k] = np.cov(training_ae[training_arhmm == k].T)\n",
    "    Sigma_infty[k] = np.cov(xs_eff_flat.T)\n",
    "    \n",
    "mu0 = np.zeros(D)\n",
    "Sigma0 = np.eye(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make effective AR matrices\n",
    "\n",
    "# Make the big A matrix\n",
    "As_eff = np.zeros((K, D * nlags, D * nlags))\n",
    "for k in range(K):\n",
    "    for l in range(nlags):\n",
    "        As_eff[k, :D, D*l:D*(l+1)] = As[k, :, l*D:(l+1)*D]\n",
    "        if l < nlags - 1:\n",
    "            As_eff[k, (l+1)*D:(l+2)*D, l*D:(l+1)*D] = np.eye(D)\n",
    "\n",
    "# plt.imshow(big_As[0])\n",
    "# plt.colorbar()\n",
    "\n",
    "# Make the big Q (covariance) matrix\n",
    "Qs_eff = np.zeros((K, D * nlags, D * nlags))\n",
    "for k in range(K):\n",
    "    Qs_eff[k, :D, :D] = Qs[k]\n",
    "    for l in range(1, nlags):\n",
    "        Qs_eff[k, l*D:(l+1)*D, l*D:(l+1)*D] = 1e-16 * np.eye(D)\n",
    "\n",
    "bs_eff = np.zeros((K, D * nlags))\n",
    "for k in range(K):\n",
    "    bs_eff[k, :D] = bs[k]\n",
    "\n",
    "# plt.imshow(big_Q[0])\n",
    "# plt.colorbar()\n",
    "\n",
    "mu0_eff = np.concatenate((mu0, mu0, mu0))\n",
    "Sigma0_eff = np.zeros((D*nlags, D*nlags))\n",
    "for l in range(nlags):\n",
    "    Sigma0_eff[l*D:(l+1)*D, l*D:(l+1)*D] = Sigma0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the information form.  It will generalize better to VI.\n",
    "from pylds.lds_messages_interface import info_E_step\n",
    "\n",
    "# Compute the info potentials for the initial condition\n",
    "def _info_params(mu0, Sigma0, As, bs, Qs, q_mu_x, q_Sigma_x, Sigma_infty, mu_infty, Ez, z_sample):\n",
    "    # parameter checking\n",
    "    T, K = Ez.shape\n",
    "    assert As.shape[0] == K and As.ndim == 3 and As.shape[1] == As.shape[2]\n",
    "    D = As.shape[1]\n",
    "    assert mu0.shape == (D,)\n",
    "    assert Sigma0.shape == (D, D)\n",
    "    assert bs.shape == (K, D)\n",
    "    assert Qs.shape == (K, D, D)\n",
    "    assert q_mu_x.shape == (T, D)\n",
    "    assert q_Sigma_x.shape == (D, D)\n",
    "    \n",
    "    # Make pseudo-inputs (all ones) for bias terms\n",
    "    inputs = np.ones((T, 1))\n",
    "    \n",
    "    # Convert initial distribution to info form\n",
    "    # (ignore normalizing constants)\n",
    "    J0 = np.linalg.inv(Sigma0)\n",
    "    h0 = J0 @ mu0\n",
    "    log_Z0 = 0\n",
    "\n",
    "    # Info dynamics parameters\n",
    "    J_pair_22 = np.linalg.inv(Qs)\n",
    "    J_pair_21 = -np.matmul(np.linalg.inv(Qs), As)\n",
    "    J_pair_11 = np.matmul(np.swapaxes(As, 1, 2), -J_pair_21)\n",
    "    mBTQiA = np.matmul(np.swapaxes(bs[:, :, None], 1, 2), J_pair_21)\n",
    "    BTQi = np.matmul(np.swapaxes(bs[:, :, None], 1, 2), J_pair_22)\n",
    "\n",
    "    # Get expected sufficient statistics by integrating over z\n",
    "    J_pair_22 = np.einsum('tk, kij -> tij', Ez[:-1], J_pair_22)\n",
    "    J_pair_21 = np.einsum('tk, kij -> tij', Ez[:-1], J_pair_21)\n",
    "    J_pair_11 = np.einsum('tk, kij -> tij', Ez[:-1], J_pair_11)\n",
    "    mBTQiA = np.einsum('tk, kij -> tij', Ez[:-1], mBTQiA)\n",
    "    BTQi = np.einsum('tk, kij -> tij', Ez[:-1], BTQi)\n",
    "    h_pair_1 = np.einsum('tu, tud -> td', inputs[:-1], mBTQiA)\n",
    "    h_pair_2 = np.einsum('tu, tud -> td', inputs[:-1], BTQi)\n",
    "    log_Z_pair = np.zeros(T-1)\n",
    "\n",
    "    # Info emission parameters\n",
    "    J_obs = (np.linalg.inv(q_Sigma_x)+100*np.eye(8*3) - np.linalg.inv(Sigma_infty))[z_sample]\n",
    "    h_recog = np.linalg.solve(q_Sigma_x, q_mu_x.T).T\n",
    "    h_infty = np.linalg.solve(Sigma_infty, mu_infty)[z_sample]\n",
    "    h_obs = h_recog + h_infty\n",
    "    log_Z_obs = np.zeros(T)\n",
    "    \n",
    "    return J0, h0, log_Z0, \\\n",
    "           J_pair_11, J_pair_21, J_pair_22, h_pair_1, h_pair_2, log_Z_pair, \\\n",
    "           J_obs, h_obs, log_Z_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  1.117832003145181e+27\n",
      "MSE:  0.8930907281577783\n",
      "MSE:  2.3412982988802176\n",
      "MSE:  0.8887133853418858\n",
      "MSE:  0.9868121822483477\n",
      "MSE:  529307924782.177\n",
      "MSE:  0.8902818462370086\n",
      "MSE:  642312.3714668679\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n",
      "MSE:  0.8878925902162449\n",
      "MSE:  297.1533571076802\n"
     ]
    }
   ],
   "source": [
    "n_iter = 100\n",
    "t=50 # which batch to use\n",
    "\n",
    "\n",
    "# Initialize q(z) with just the learned recognition potential\n",
    "log_qz = recog_z.predict_log_proba(ns_window[t])\n",
    "q_mu_x = recog_x.predict(ns_window[t])\n",
    "q_Sigma_x = np.cov((xs_eff[t] - q_mu_x).T)\n",
    "\n",
    "z_potential = log_qz - np.log(pz_infty)\n",
    "Ez, _, _ = hmm_expected_states(np.log(pz_infty), np.log(P)[None, :, :], z_potential)\n",
    "z_sample = hmm_sample(np.log(pz_infty), np.log(P)[None, :, :], z_potential)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # compute the expected value of x given z and the observation potential\n",
    "    _, Ex, _, _ = info_E_step(\n",
    "        *_info_params(mu0_eff, Sigma0_eff, As_eff, bs_eff, Qs_eff, \n",
    "                      q_mu_x, q_Sigma_x, Sigma_infty, mu_infty, Ez, z_sample)\n",
    "    )\n",
    "    \n",
    "    # Update z, now including the dynamics potential\n",
    "    # TODO: Should really include the covariance of x in this update too\n",
    "    z_dyn_potential = np.column_stack(\n",
    "        [multivariate_normal_logpdf(Ex[1:], Ex[:-1] @ A.T + b, Q) \n",
    "         for A, b, Q in zip(As_eff, bs_eff, Qs_eff)])\n",
    "    z_dyn_potential = np.row_stack((np.zeros(K), z_dyn_potential))\n",
    "    z_potential = log_qz - np.log(pz_infty) + z_dyn_potential\n",
    "    Ez, _, _ = hmm_expected_states(np.log(pz_infty), np.log(P)[None, :, :], z_potential)\n",
    "\n",
    "    print(\"MSE: \", np.mean((Ex[:,:D] - xs[t])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behavenet",
   "language": "python",
   "name": "behavenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
