{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting BehaveNet to example datasets\n",
    "\n",
    "This series of notebooks will walk you through how to download example datasets and fit the various models in the BehaveNet toolbox.\n",
    "\n",
    "Before beginning, first make sure that you have properly installed the BehaveNet package and environment by following the instructions [here](https://behavenet.readthedocs.io/en/latest/source/installation.html) (specifically, (1) setting up the Anaconda virtual environment; and (2) installing the `BehaveNet` and `ssm` packages).\n",
    "\n",
    "To illustrate the use of BehaveNet we will use an example dataset from [Musall et al 2019](https://www.nature.com/articles/s41593-019-0502-4), which is also one of the datasets used in the original [BehaveNet paper](https://openreview.net/forum?id=ByxMASrlUB).\n",
    "\n",
    "Briefly, a head-fixed mouse performed a visual decision-making task while neural activity across dorsal cortex was optically recorded using widefield calcium imaging. We used the [LocaNMF](https://www.biorxiv.org/content/10.1101/650093v2) decomposition approach to extract signals from the calcium imaging video. Behavioral data was recorded using two cameras: one side view and one bottom view. Grayscale video frames were downsampled to 128x128 pixels. Data consists of 1126 trials across two sessions in the same mouse, with 189 frames per trial (30 Hz framerate). Neural activity was acquired at the same frame rate.\n",
    "\n",
    "The data are stored on the Cold Spring Harbor data repository; you will download this data after setting some user paths.\n",
    "\n",
    "**Note**: make sure that you are running the `behavenet` ipython kernel - you should see the current ipython kernel name in the upper right hand corner of this notebook. If it is not `behavenet` (for example it might be `Python 3`) then change it using the dropdown menus above: `Kernel > Change kernel > behavenet`. If you do not see `behavenet` as an option see [here](https://behavenet.readthedocs.io/en/latest/source/installation.html#environment-setup).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Contents\n",
    "* [Set user paths](#0.-Set-user-paths)\n",
    "* [Download the data](#1.-Download-the-data)\n",
    "* [Add dataset hyperparameters](#2.-Add-dataset-hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set user paths\n",
    "First set the paths to the directories where data, results, and figures will be stored on your local machine. Note that the data is ~7.5GB, so make sure that your data directory has enough space.\n",
    "\n",
    "A note about the BehaveNet path structure: every dataset is uniquely identified by a lab id, experiment id, animal id, and session id. Paths to data and results contain directories for each of these id types. For example, a sample data path will look like `/home/user/data/lab_id/expt_id/animal_id/session_id/data.hdf5`. In this case the base data directory is `/home/user/data/`.\n",
    "\n",
    "The downloaded zip file contains two datasets, which will automatically be saved as:\n",
    "* `data_dir/musall/vistrained/mSM36/05-Dec-2017/data.hdf5`\n",
    "* `data_dir/musall/vistrained/mSM36/07-Dec-2017/data.hdf5`\n",
    "\n",
    "Additionally, the zip file contains already trained convolutional neural networks (the most time consuming step of the pipeline), which will automatically be saved in the directories:\n",
    "* `results_dir/musall/vistrained/mSM36/05-Dec-2017/ae/conv/09_latents/ae-example/`\n",
    "* `results_dir/musall/vistrained/mSM36/07-Dec-2017/ae/conv/09_latents/ae-example/`\n",
    "* `results_dir/musall/vistrained/mSM36/multisession-00/ae/conv/09_latents/ae-example/`\n",
    "\n",
    "The first two directories contain AEs trained on the individual sessions; the third directory contains an AE trained on both sessions simultaneously.\n",
    "\n",
    "To set the user paths, run the cell below.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from behavenet import setup\n",
    "setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory file is stored in your user home directory; this is a json file that can be edited in a text editor at any time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the data\n",
    "Run the cell below; data and results will be stored in the directories provided in the previous step.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import zipfile as zf\n",
    "\n",
    "# download zip file\n",
    "zfile = '/media/mattw/data/behavenet_example_data/behavenet_example_data.zip'\n",
    "z = zf.ZipFile(zfile)\n",
    "\n",
    "# extract data\n",
    "data_dir = get_user_dir('data')\n",
    "print('extracting data to %s...' % data_dir, end='')\n",
    "for file in z.namelist():\n",
    "    if file.startswith('behavenet_ex/data/'):\n",
    "        z.extract(file, data_dir)\n",
    "# clean up paths\n",
    "shutil.move(os.path.join(data_dir, 'behavenet_ex', 'data', 'musall'), data_dir)\n",
    "shutil.rmtree(os.path.join(data_dir, 'behavenet_ex'))\n",
    "print('done')\n",
    "\n",
    "# extract results\n",
    "results_dir = get_user_dir('save')\n",
    "print('extracting results to %s...' % data_dir, end='')\n",
    "for file in z.namelist():\n",
    "    if file.startswith('behavenet_ex/results/'):\n",
    "        z.extract(file, results_dir)\n",
    "# clean up paths\n",
    "shutil.move(os.path.join(results_dir, 'behavenet_ex', 'results', 'musall'), results_dir)\n",
    "shutil.rmtree(os.path.join(results_dir, 'behavenet_ex'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import zipfile as zf\n",
    "from behavenet import get_user_dir, make_dir_if_not_exists\n",
    "\n",
    "tmp_zip_file = os.path.join(get_user_dir('data'), 'tmp.zip')\n",
    "make_dir_if_not_exists(tmp_zip_file)\n",
    "\n",
    "url = 'https://drive.google.com/open?id=13nqHu_UA2eOr6cWImKmfG7E3Y_eEiwOz'\n",
    "\n",
    "import requests\n",
    "print('beginning file download')\n",
    "r = requests.get(url, stream=True)\n",
    "z = zf.ZipFile(io.BytesIO(r.content))\n",
    "# z.extractall(get_user_dir('data'))\n",
    "# with open(tmp_zip_file, 'wb') as f:\n",
    "#     f.write(r.content)\n",
    "# wget.download(url, tmp_zip_file)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add dataset hyperparameters\n",
    "The last step is to save some of the dataset hyperparameters in their own json file. This is used to simplify command line arguments to model fitting functions. The relevant parameters and their values are:\n",
    "\n",
    "* `lab or experimenter name` (musall - note: quotes are not needed around strings)\n",
    "* `experiment name` (vistrained)\n",
    "* `example animal name` (mSM36)\n",
    "* `example session name` (05-Dec-2017)\n",
    "* `trial splits` (8;1;1;0) - this is how trials will be split among training, validation, testing, and gap trials, respectively. Typically we use training data to train the models; validation data to choose the best model from a collection of models using different hyperparameters; test data to produce plots and videos; and gap trials can optionally be inserted between training, validation, and test trials if desired.\n",
    "* `x pixels` (128)\n",
    "* `y pixels` (128)\n",
    "* `input channels` (2) - this can refer to color channels (for RGB data) and/or multiple camera views, which should be concatenated along the color channel dimension. In the Musall dataset we use grayscale images from two camera views, so a trial with 189 frames will have a block of video data of shape (189, 2, 128, 128)\n",
    "* `use output mask` (False) - an optional output mask can be applied to each video frame if desired; these output masks must also be stored in the data.hdf5 files as masks.\n",
    "* `frame rate` (30) - in Hz; behavenet assumes that the video data and neural data are binned at the same temporal resolution\n",
    "* `neural data type` (ca) - either ca for 2-photon/widefield data, or spikes for ephys data. This parameter controls the noise distribution for encoding models, as well as several other model hyperparameters.\n",
    "\n",
    "To save these, run the cell below and enter them one at a time.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from behavenet import add_dataset\n",
    "add_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behavenet",
   "language": "python",
   "name": "behavenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
