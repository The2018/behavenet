{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting BehaveNet to an example dataset\n",
    "\n",
    "This series of notebooks will walk you through how to download an example dataset and fit the various models in the BehaveNet toolbox.\n",
    "\n",
    "Before beginning, first make sure that you have properly installed the BehaveNet package and environment by following the instructions [here](https://behavenet.readthedocs.io/en/latest/source/installation.html). Specifically, (1) set up the Anaconda virtual environment; and (2) install the `BehaveNet` and `ssm` packages. You do not need to set user paths at this time (this will be covered below).\n",
    "\n",
    "To illustrate the use of BehaveNet we will use an example dataset from [Musall et al 2019](https://www.nature.com/articles/s41593-019-0502-4), which is also one of the datasets used in the original [BehaveNet paper](https://openreview.net/forum?id=ByxMASrlUB).\n",
    "\n",
    "Briefly, a head-fixed mouse performed a visual decision-making task while neural activity across dorsal cortex was optically recorded using widefield calcium imaging. We used the [LocaNMF](https://www.biorxiv.org/content/10.1101/650093v2) decomposition approach to extract signals from the calcium imaging video. Behavioral data was recorded using two cameras: one side view and one bottom view. Grayscale video frames were downsampled to 128x128 pixels. Data consists of 1126 trials across two sessions in the same mouse, with 189 frames per trial (30 Hz framerate). Neural activity was acquired at the same frame rate.\n",
    "\n",
    "The data are stored on the Cold Spring Harbor data repository; you will download this data after setting some user paths.\n",
    "\n",
    "**Note**: make sure that you are running the `behavenet` ipython kernel - you should see the current ipython kernel name in the upper right hand corner of this notebook. If it is not `behavenet` (for example it might be `Python 3`) then change it using the dropdown menus above: `Kernel > Change kernel > behavenet`. If you do not see `behavenet` as an option see [here](https://behavenet.readthedocs.io/en/latest/source/installation.html#environment-setup).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Contents\n",
    "* [Set user paths](#0.-Set-user-paths)\n",
    "* [Download the data](#1.-Download-the-data)\n",
    "* [Add dataset hyperparameters](#2.-Add-dataset-hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set user paths\n",
    "First set the paths to the directories where data, results, and figures will be stored on your local machine. Note that the data is ~7.5GB, so make sure that your data directory has enough space.\n",
    "\n",
    "A note about the BehaveNet path structure: every dataset is uniquely identified by a lab id, experiment id, animal id, and session id. Paths to data and results contain directories for each of these id types. For example, a sample data path will look like `/home/user/data/lab_id/expt_id/animal_id/session_id/data.hdf5`. In this case the base data directory is `/home/user/data/`.\n",
    "\n",
    "The downloaded zip file contains two datasets, which will automatically be saved as:\n",
    "* `data_dir/musall/vistrained/mSM36/05-Dec-2017/data.hdf5`\n",
    "* `data_dir/musall/vistrained/mSM36/07-Dec-2017/data.hdf5`\n",
    "\n",
    "Additionally, the zip file contains already trained convolutional neural networks (the most time consuming step of the pipeline), which will automatically be saved in the directories:\n",
    "* `results_dir/musall/vistrained/mSM36/05-Dec-2017/ae/conv/09_latents/ae-example/`\n",
    "* `results_dir/musall/vistrained/mSM36/07-Dec-2017/ae/conv/09_latents/ae-example/`\n",
    "* `results_dir/musall/vistrained/mSM36/multisession-00/ae/conv/09_latents/ae-example/`\n",
    "\n",
    "The first two directories contain AEs trained on the individual sessions; the third directory contains an AE trained on both sessions simultaneously.\n",
    "\n",
    "To set the user paths, run the cell below.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from behavenet import setup\n",
    "setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory file is stored in your user home directory; this is a json file that can be updated in a text editor at any time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the data\n",
    "Run the cell below; data and results will be stored in the directories provided in the previous step.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import requests\n",
    "import zipfile as zf\n",
    "from behavenet import get_user_dir\n",
    "\n",
    "url = 'http://labshare.cshl.edu/shares/library/repository/38599/behavenet_example_data.zip'\n",
    "\n",
    "print('Downloading data - this may take several minutes')\n",
    "\n",
    "# fetch data from CSHL data repository\n",
    "print('fetching data from url...', end='')\n",
    "r = requests.get(url, stream=True)\n",
    "z = zf.ZipFile(io.BytesIO(r.content))\n",
    "print('done')\n",
    "\n",
    "# extract data\n",
    "data_dir = get_user_dir('data')\n",
    "print('extracting data to %s...' % data_dir, end='')\n",
    "for file in z.namelist():\n",
    "    if file.startswith('behavenet_ex/data/'):\n",
    "        z.extract(file, data_dir)\n",
    "# clean up paths\n",
    "shutil.move(os.path.join(data_dir, 'behavenet_ex', 'data', 'musall'), data_dir)\n",
    "shutil.rmtree(os.path.join(data_dir, 'behavenet_ex'))\n",
    "print('done')\n",
    "\n",
    "# extract results\n",
    "results_dir = get_user_dir('save')\n",
    "print('extracting results to %s...' % results_dir, end='')\n",
    "for file in z.namelist():\n",
    "    if file.startswith('behavenet_ex/results/'):\n",
    "        z.extract(file, results_dir)\n",
    "# clean up paths\n",
    "shutil.move(os.path.join(results_dir, 'behavenet_ex', 'results', 'musall'), results_dir)\n",
    "shutil.rmtree(os.path.join(results_dir, 'behavenet_ex'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add dataset hyperparameters\n",
    "The last step is to save some of the dataset hyperparameters in their own json file. This is used to simplify command line arguments to model fitting functions. The relevant parameters and their values are:\n",
    "\n",
    "* `lab or experimenter name` (musall - note: quotes are not needed around strings)\n",
    "* `experiment name` (vistrained)\n",
    "* `example animal name` (mSM36)\n",
    "* `example session name` (05-Dec-2017)\n",
    "* `input channels` (2) - this can refer to color channels (for RGB data) and/or multiple camera views, which should be concatenated along the color channel dimension. In the Musall dataset we use grayscale images from two camera views, so a trial with 189 frames will have a block of video data of shape (189, 2, 128, 128)\n",
    "* `y pixels` (128)\n",
    "* `x pixels` (128)\n",
    "* `use output mask` (False) - an optional output mask can be applied to each video frame if desired; these output masks must also be stored in the data.hdf5 files as masks.\n",
    "* `frame rate` (30) - in Hz; behavenet assumes that the video data and neural data are binned at the same temporal resolution\n",
    "* `neural data type` (ca) - either ca for 2-photon/widefield data, or spikes for ephys data. This parameter controls the noise distribution for encoding models, as well as several other model hyperparameters.\n",
    "\n",
    "To save these, run the cell below and enter them one at a time.\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the following information about the dataset:\n",
      "name of experimenter/lab (str): musall\n",
      "name of experiment (str): vistrained\n",
      "example animal name (str): mSM36\n",
      "example session name (str): 05-Dec-2017\n",
      "Please enter the following information about the behavioral video:\n",
      "number of x pixels (int): 128\n",
      "number of y pixels (int): 128\n",
      "number of camera views (int): 2\n",
      "are you applying any masks to the video? (True/False): False\n",
      "frame rate of video (Hz) (float): 30\n",
      "Please enter the following information about the neural data:\n",
      "neural data type (str - spikes or ca): ca\n",
      "Dataset params are now stored in /home/mattw/.behavenet/musall_vistrained_params\n"
     ]
    }
   ],
   "source": [
    "from behavenet import add_dataset\n",
    "add_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to start fitting and analyzing models; before doing so, however, it might be useful to read more about the basics of the BehaveNet package [here](https://behavenet.readthedocs.io/en/latest/source/user_guide.intro.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behavenet2",
   "language": "python",
   "name": "behavenet2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
