{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datta.neural_decoding_models.base_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4bd67b47c3c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscpm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_decoding_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datta.neural_decoding_models.base_model'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable, Function\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import scipy.misc as scpm\n",
    "import math\n",
    "from datta.neural_decoding_models.base_model import BaseModel\n",
    "import h5py\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### LOAD PCA DATA ####\n",
    "n_mice=17\n",
    "h5_temp = h5py.File('whitened_clean_pca.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3382c9d2-19a2-4b91-83c5-e1f0d1ead9f1\n",
      "5cb78054-857e-4e1b-9d57-f6a9c6538be4\n",
      "62640038-b31a-4503-be1e-ace7a20fa397\n",
      "71a57228-e96a-4c14-a3df-093d14c8f82d\n",
      "74b6aee1-d39c-48b2-8524-776cae011514\n",
      "913391df-a13d-44fe-93fa-3ff89e682334\n",
      "918c32b1-4c50-42f9-bbb5-2fa0ead9a451\n",
      "919992b5-9001-4168-ac5c-f6b7354e02e8\n",
      "94dfc8af-6c1d-4a2a-9242-dd4eee271118\n",
      "bff6563f-4db1-4bfe-890e-0b4fc377a479\n",
      "c20834ea-f2a7-4fdd-8d6c-cd800687317d\n",
      "c4a42b03-f1ff-41c5-af11-1354c1066e9d\n",
      "c6a254b4-3812-490e-a782-09c8d4ec1472\n",
      "db4f4e84-9892-4af4-961f-2d8ecfb732c2\n",
      "e0287346-6a87-4593-8601-feb0583d7ea8\n",
      "e116263b-3600-455f-806e-4f4192aa33cd\n",
      "e7fdeb9c-9bcd-416a-bc13-7fe793741468\n",
      "f2d79bd4-9991-4ac1-932f-dadcc2f3a4ed\n",
      "fa4ce1b5-0522-448b-a7d4-f2fc74f94ace\n",
      "fe6b2573-998f-4eff-9f2d-a66c927dde4c\n"
     ]
    }
   ],
   "source": [
    "for k in h5_temp.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't use mice with nans\n",
    "uuids_all = list(h5_temp.keys())\n",
    "uuids = copy.deepcopy(uuids_all)\n",
    "delete_inds=[]\n",
    "for i in range(len(uuids_all)):\n",
    "   # print(np.sum(np.isnan(h5_temp[uuids_all[i]][:])))\n",
    "    if np.sum(np.isnan(h5_temp[uuids_all[i]][:]))>0:\n",
    "        delete_inds.append(i)\n",
    "for ii in sorted(delete_inds,reverse=True):\n",
    "    del uuids[ii]\n",
    "    \n",
    "for i in range(n_mice):\n",
    "   # print(uuids[i])\n",
    "    if np.sum(np.isnan(h5_temp[uuids[i]][:])) > 0:\n",
    "        print('ERROR: NANS STILL PRESENT')\n",
    "        ver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### CREATE DATA GENERATORS ####\n",
    "\n",
    "def data_generator(pca_file, uuids, batch_size,n_mice):\n",
    "    \n",
    "    pca_file = h5_temp\n",
    "    \n",
    "    total_batches=0\n",
    "    n_batches = [None]*n_mice\n",
    "\n",
    "    for i_mouse in range(n_mice):\n",
    "        length_mouse = pca_file[uuids[i_mouse]].shape[0]\n",
    "        n_batches[i_mouse] = np.floor(length_mouse/batch_size)\n",
    "\n",
    "    total_batches = int(np.sum(n_batches))\n",
    "\n",
    "    batch_inds = np.zeros((int(total_batches),2))\n",
    "    i_pos=0\n",
    "    \n",
    "    for i_mouse in range(n_mice):\n",
    "        for i_batch in range(int(n_batches[i_mouse])):\n",
    "            batch_inds[i_pos,0] = i_mouse\n",
    "            batch_inds[i_pos,1] = i_batch\n",
    "            i_pos+=1\n",
    "\n",
    "    loop_vec = np.arange(total_batches)\n",
    "    \n",
    "    \n",
    "    for i_epoch in range(1000):\n",
    "        np.random.shuffle(loop_vec)\n",
    "        for ii in loop_vec:\n",
    "\n",
    "            i_mouse = int(batch_inds[ii,0])\n",
    "            which_batch = int(batch_inds[ii,1])\n",
    "\n",
    "            yield pca_file[uuids[i_mouse]][which_batch*batch_size:(which_batch+1)*batch_size], i_mouse, which_batch #, behavioral_labels[i_mouse][which_bucket][which_batch*batch_size:(which_batch+1)*batch_size],depth[i_mouse][which_bucket][which_batch*batch_size:(which_batch+1)*batch_size],i_mouse,which_bucket,which_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901\n"
     ]
    }
   ],
   "source": [
    "pca_file = h5_temp\n",
    "\n",
    "total_batches=0\n",
    "n_batches = [None]*n_mice\n",
    "\n",
    "for i_mouse in range(n_mice):\n",
    "    length_mouse = pca_file[uuids[i_mouse]].shape[0]\n",
    "    n_batches[i_mouse] = np.floor(length_mouse/batch_size)\n",
    "    \n",
    "total_batches = int(np.sum(n_batches))\n",
    "nb_tng_batches = total_batches\n",
    "print(nb_tng_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_gen = data_generator(h5_temp,uuids,batch_size,n_mice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_sum_exp(value, dim=None, keepdim=False):\n",
    "    \"\"\"Numerically stable implementation of the operation\n",
    "    value.exp().sum(dim, keepdim).log()\n",
    "    \"\"\"\n",
    "    # TODO: torch.max(value, dim=None) threw an error at time of writing\n",
    "    if dim is not None:\n",
    "        m, _ = torch.max(value, dim=dim, keepdim=True)\n",
    "        value0 = value - m\n",
    "        if keepdim is False:\n",
    "            m = m.squeeze(dim)\n",
    "        return m + torch.log(torch.sum(torch.exp(value0),\n",
    "                                       dim=dim, keepdim=keepdim))\n",
    "    else:\n",
    "        m = torch.max(value)\n",
    "        sum_exp = torch.sum(torch.exp(value - m))\n",
    "        return m + torch.log(sum_exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Losses\n",
    "\n",
    "from datta.messages.torchhmm import hmm_marginal_likelihood\n",
    "\n",
    "def arhmm_log_probability(z, h, log_Ps, As, bs, Qs, nlags):\n",
    "    \"\"\"\n",
    "    Computes p(h | A, C) = \\sum_z p(h, z | Ps, As, bs, Qs)\n",
    "    using message passing algorithm for hidden Markov models.\n",
    "    \n",
    "    p(h, z | Ps, As, bs, Qs) = \\prod_t N(h[t] | As[z[t]] h[t-1] + bs[z[t]], Qs[z[t]])\n",
    "                                    \\times P[z[t] | z[t-1]]\n",
    "    \n",
    "    @param z       T         array of integer valued states 1, ..., K\n",
    "    @param h       T x H     array of observations\n",
    "    @param log_Ps: T x K x K set of log transition matrices for each time step t\n",
    "    @param As:     K x H x H set of dynamics matrices\n",
    "    @param bs:     K x H     set of dynamics biases\n",
    "    @param log_Qs: K x H     matrix of (diagonal) log variances of h given z\n",
    "    \"\"\"\n",
    "    T, H = h.shape\n",
    "    K = log_Ps.shape[1]\n",
    "    assert log_Ps.shape == (T-1, K, K)\n",
    "    assert As.shape == (K, H*nlags, H)\n",
    "    assert bs.shape == (K, H)\n",
    "    assert Qs.shape == (K, H)\n",
    "    \n",
    "    # Initialize output \n",
    "    lp = 0\n",
    "    \n",
    "    # Assume a uniform initial distribution\n",
    "    log_pi0 = -math.log(K) * torch.ones(K)\n",
    "\n",
    "    # Make sure transition matrices are normalized\n",
    "    log_Ps = log_Ps - log_sum_exp(log_Ps, dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute log p(theta)\n",
    "    alpha = 6\n",
    "    kappa = 1e8\n",
    "    Ps = torch.exp(log_Ps)\n",
    "    for i_state in range(K):\n",
    "        concentration = alpha*torch.ones(K) / K\n",
    "        concentration[i_state] += kappa\n",
    "        dirichlet_dist = torch.distributions.dirichlet.Dirichlet(concentration)\n",
    "        lp += torch.sum(dirichlet_dist.log_prob(Ps[i_state]))\n",
    "    \n",
    "    # Compute log p(z | theta)\n",
    "    lp += torch.sum(log_Ps[torch.range(0, T-2).long(), z[:-1], z[1:]])\n",
    "    \n",
    "    # Precompute the log likelihoods\n",
    "    # N(h[t+1] | As[z[t]] h[t] + bs[z[t]], Qs[z[t]])    \n",
    "    E_hs = torch.matmul(torch.cat(([h[nlags-1-i:T-1-i] for i in range(nlags,-1,-1)]),dim=1).contiguous().view(T-nlags, 1, 1, H*nlags), As).squeeze(2) + bs       # T-1 x K x H\n",
    "    E_hs = torch.cat((bs.view(1, K, H).repeat(nlags,1,1) , E_hs), 0)                             # T x K x H\n",
    "\n",
    "    # Note: you could for-loop over lags and add up the contributions for each one\n",
    "    #       this would avoid allocating and copying memory\n",
    "    # E_hs = torch.zeros(T, K, H)\n",
    "    # E_hs += bs\n",
    "    # for lag in range(nlags):\n",
    "    #    ... \n",
    "        \n",
    "    lls = -0.5 * torch.sum((h.unsqueeze(1) - E_hs)**2 / Qs, dim=2)         # T x K\n",
    "    lls += -0.5 * torch.sum(math.log(2 * math.pi) + torch.log(Qs), dim=1)  # K \n",
    "\n",
    "    # Compute log p(x | z, theta)\n",
    "    lp += torch.sum(lls[torch.range(0, T-1).long(), z])\n",
    "\n",
    "    return lp\n",
    "\n",
    "def DeepInputDrivenARHMM_EM_loss(zs, h, log_Ps, As, bs, Qs, nlags=1):\n",
    "    \"\"\"\n",
    "    zs is a (S, T) LongTensor with S samples of the discrete states\n",
    "    \"\"\"\n",
    "    K, D, _ = As.shape\n",
    "    S, _ = zs.shape\n",
    "    \n",
    "    # log prob is likelihood plus prior\n",
    "    lp = 0\n",
    "    for s in range(S):\n",
    "        lp += arhmm_log_probability(zs[s], h, log_Ps, As, bs, Qs, nlags) / S\n",
    "    \n",
    "    return -lp / (h.shape[0] * h.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Losses\n",
    "\n",
    "from datta.messages.torchhmm import hmm_marginal_likelihood\n",
    "\n",
    "def arhmm_marginal_likelihood(h, log_Ps, As, bs, Qs, nlags):\n",
    "    \"\"\"\n",
    "    Computes p(h | A, C) = \\sum_z p(h, z | Ps, As, bs, Qs)\n",
    "    using message passing algorithm for hidden Markov models.\n",
    "    \n",
    "    p(h, z | Ps, As, bs, Qs) = \\prod_t N(h[t] | As[z[t]] h[t-1] + bs[z[t]], Qs[z[t]])\n",
    "                                    \\times P[z[t] | z[t-1]]\n",
    "    \n",
    "    @param H       T x H     array of observations\n",
    "    @param log_Ps: T x K x K set of log transition matrices for each time step t\n",
    "    @param As:     K x H x H set of dynamics matrices\n",
    "    @param bs:     K x H     set of dynamics biases\n",
    "    @param log_Qs: K x H     matrix of (diagonal) log variances of h given z\n",
    "    \"\"\"\n",
    "    T, H = h.shape\n",
    "    K = log_Ps.shape[1]\n",
    "    assert log_Ps.shape == (T-1, K, K)\n",
    "    assert As.shape == (K, H*nlags, H)\n",
    "    assert bs.shape == (K, H)\n",
    "    assert Qs.shape == (K, H)\n",
    "\n",
    "    # Assume a uniform initial distribution\n",
    "    log_pi0 = -math.log(K) * torch.ones(K)\n",
    "\n",
    "    # Make sure transition matrices are normalized\n",
    "    log_Ps = log_Ps - log_sum_exp(log_Ps, dim=-1, keepdim=True)\n",
    "\n",
    "    # Precompute the log likelihoods\n",
    "    # N(h[t+1] | As[z[t]] h[t] + bs[z[t]], Qs[z[t]])\n",
    "    #AsT = As.transpose(1, 2)\n",
    "    \n",
    "    E_hs = torch.matmul(torch.cat(([h[nlags-1-i:T-1-i] for i in range(nlags,-1,-1)]),dim=1).contiguous().view(T-nlags, 1, 1, H*nlags), As).squeeze(2) + bs       # T-1 x K x H\n",
    "    #E_hs = torch.matmul(h[:-1].contiguous().view(T-1, 1, 1, H), As).squeeze(2) + bs       # T-1 x K x H\n",
    "    E_hs = torch.cat((bs.view(1, K, H).repeat(nlags,1,1) , E_hs), 0)                             # T x K x H\n",
    "    ## TO DO: padding with nlags of b, should I do something better here, ignore lags or pad with A*what there is\n",
    "    lls = -0.5 * torch.sum((h.unsqueeze(1) - E_hs)**2 / Qs, dim=2)  # T x K\n",
    "    lls += -0.5 * torch.sum(math.log(2 * math.pi) + torch.log(Qs), dim=1)            # K \n",
    "\n",
    "    return hmm_marginal_likelihood(log_pi0, log_Ps, lls)\n",
    "\n",
    "def DeepInputDrivenARHMM_loss(h, log_Ps, As, bs, Qs, nlags=1):\n",
    "    K, D, _ = As.shape\n",
    "    alpha = 6\n",
    "    kappa = 1e8\n",
    "    # Make sure transition matrices are normalized\n",
    "    log_Ps = log_Ps - log_sum_exp(log_Ps, dim=-1, keepdim=True)\n",
    "    Ps = torch.exp(log_Ps)\n",
    "    for i_state in range(K):\n",
    "        concentration = alpha*torch.ones(K) / K\n",
    "        concentration[i_state] += kappa\n",
    "        dirichlet_dist = torch.distributions.dirichlet.Dirichlet(concentration)\n",
    "        if i_state==0:\n",
    "            prior_trans = torch.sum(dirichlet_dist.log_prob(Ps[i_state]))\n",
    "        else:\n",
    "            prior_trans += torch.sum(dirichlet_dist.log_prob(Ps[i_state]))\n",
    "    #Qs = torch.ones((K,D)) #torch.eye(D).repeat(K, 1, 1)\n",
    "    \n",
    "    # log prob is likelihood plus prior\n",
    "    log_prob = arhmm_marginal_likelihood(h, log_Ps, As, bs, Qs, nlags) + prior_trans\n",
    "    \n",
    "    return -log_prob/(h.shape[0]*h.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set up model\n",
    "\n",
    "from datta.messages.torchhmm import hmm_sample\n",
    "\n",
    "class ARHMM(nn.Module):\n",
    "    def __init__(self, K, H, kappa, batch_size, nlags):\n",
    "        super(ARHMM, self).__init__()\n",
    "        self.n_discrete_states = K\n",
    "        self.latent_dim_size_h = H\n",
    "        self.kappa=kappa\n",
    "        self.batch_size=batch_size\n",
    "        self.nlags = nlags\n",
    "        self.__build_model()\n",
    "    \n",
    "    def __build_model(self):\n",
    "\n",
    "        # Emission parameters\n",
    "        self.As = nn.Parameter(torch.zeros((self.n_discrete_states, self.latent_dim_size_h*self.nlags, self.latent_dim_size_h)))\n",
    "        self.bs = nn.Parameter(torch.zeros((self.n_discrete_states, self.latent_dim_size_h)))\n",
    "        #self.inv_softplus_Qs = nn.Parameter(torch.zeros((self.n_discrete_states, self.latent_dim_size_h)))\n",
    "        self.inv_softplus_Qs = nn.Parameter(torch.ones((self.n_discrete_states, self.latent_dim_size_h)))\n",
    "\n",
    "        # Transition bias network (x -> transition bias + transition prob)\n",
    "        self.log_transition_proba = \\\n",
    "                nn.Parameter(torch.log(\n",
    "                self.kappa * torch.eye(self.n_discrete_states) + (1-self.kappa) / self.n_discrete_states * torch.ones((self.n_discrete_states, self.n_discrete_states))))\n",
    "\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self):\n",
    "        log_Ps = self.log_transition_proba\n",
    "        log_Ps = log_Ps.unsqueeze(0).repeat(self.batch_size-1,1,1)\n",
    "        \n",
    "        return log_Ps, self.As, self.bs, self.softplus(self.inv_softplus_Qs) #E_y, var_y, mu_post, var_post, h, log_Ps, self.As, self.bs, self.softplus(self.inv_softplus_Qs)\n",
    "\n",
    "    def loss(self, h, log_Ps, As, bs, Qs):\n",
    "        return DeepInputDrivenARHMM_loss(h, log_Ps, As, bs, Qs, self.nlags) \n",
    "\n",
    "    def sample_states(self, h, num_samples=1):\n",
    "        log_pi0 = ...\n",
    "        log_Ps = ...\n",
    "        lls = ...\n",
    "        \n",
    "        # Concatenate into an S x T LongTensor\n",
    "        return torch.cat([hmm_sample(log_pi0, log_Ps, lls) for _ in range(num_samples)])\n",
    "    \n",
    "    def EM_loss(self, zs, h, log_Ps, As, bs, Qs):\n",
    "        return DeepInputDrivenARHMM_EM_loss(zs, h, log_Ps, As, bs, Qs, self.nlags) \n",
    "\n",
    "    \n",
    "    def get_expected_states(self, h, log_Ps, As, bs, Qs):\n",
    "\n",
    "        T, H = h.shape\n",
    "        K = log_Ps.shape[1]\n",
    "\n",
    "        # Get log likelihoods\n",
    "        # Make sure transition matrices are normalized\n",
    "        log_Ps = log_Ps - log_sum_exp(log_Ps, dim=-1, keepdim=True)\n",
    "        log_PsT = log_Ps.transpose(1, 2)\n",
    "\n",
    "        #AsT = As.transpose(1, 2)\n",
    "        E_hs = torch.matmul(torch.cat(([h[self.nlags-1-i:T-1-i] for i in range(self.nlags,-1,-1)]),dim=1).contiguous().view(T-self.nlags, 1, 1, H*self.nlags), As).squeeze(2) + bs       # T-1 x K x H\n",
    "        #E_hs = torch.matmul(h[:-1].contiguous().view(T-1, 1, 1, H), As).squeeze(2) + bs       # T-1 x K x H\n",
    "        E_hs = torch.cat((bs.view(1, K, H).repeat(self.nlags,1,1) , E_hs), 0)                             # T x K x H\n",
    "        \n",
    "       # E_hs = torch.matmul(h[:-1].contiguous().view(T-1, 1, 1, H), As).squeeze(2) + bs       # T-1 x K x H\n",
    "       # E_hs = torch.cat((bs.view(1, K, H), E_hs), 0)                             # T x K x H\n",
    "        #lls = -0.5 * torch.sum((h.unsqueeze(1) - E_hs)**2 / Qs, dim=2)  # T x K\n",
    "        #lls += -0.5 * torch.sum(math.log(2 * math.pi) + torch.log(Qs), dim=1)            # K \n",
    "        lls = -0.5 * torch.sum((h.unsqueeze(1) - E_hs)**2 / Qs, dim=2)  # T x K\n",
    "        lls += -0.5 * torch.sum(math.log(2 * math.pi) + torch.log(Qs), dim=1)            # K \n",
    "        \n",
    "#         lls = torch.zeros(T,K)\n",
    "#         for i_t in range(T):\n",
    "#             for i_k in range(K):\n",
    "#                 normal_dist = torch.distributions.multivariate_normal.MultivariateNormal(E_hs[i_t,i_k],covariance_matrix=Qs[i_k])\n",
    "#                 lls[i_t,i_k] = normal_dist.log_prob(h[i_t])\n",
    "                \n",
    "        # Forwards pass\n",
    "        alpha = torch.zeros_like(lls)\n",
    "        alpha[0] = -math.log(self.n_discrete_states) + lls[0] \n",
    "        for t in range(alpha.shape[0]-1):\n",
    "            alpha[t+1] = log_sum_exp(alpha[t] + log_PsT[t],dim=1) + lls[t+1]\n",
    "\n",
    "        # Backwards pass\n",
    "        beta = torch.zeros_like(lls)\n",
    "        for t in range(beta.shape[0]-2,-1,-1):\n",
    "            beta[t] = log_sum_exp(log_Ps[t]+beta[t+1]+lls[t+1],dim=1)\n",
    "\n",
    "        # Combine to get posterior over z\n",
    "        expected_z = alpha+beta\n",
    "        expected_z -= expected_z.max(1)[0].view((-1,1))\n",
    "        expected_z = torch.exp(expected_z)\n",
    "        expected_z /= expected_z.sum(1).view((-1,1))\n",
    "\n",
    "        return expected_z\n",
    "    \n",
    "    def get_expected_states_covmat(self, h, log_Ps, As, bs, Qs):\n",
    "\n",
    "        T, H = h.shape\n",
    "        K = log_Ps.shape[1]\n",
    "\n",
    "        # Get log likelihoods\n",
    "        # Make sure transition matrices are normalized\n",
    "        log_Ps = log_Ps - log_sum_exp(log_Ps, dim=-1, keepdim=True)\n",
    "        log_PsT = log_Ps.transpose(1, 2)\n",
    "\n",
    "        #AsT = As.transpose(1, 2)\n",
    "        E_hs = torch.matmul(torch.cat(([h[self.nlags-1-i:T-1-i] for i in range(self.nlags,-1,-1)]),dim=1).contiguous().view(T-self.nlags, 1, 1, H*self.nlags), As).squeeze(2) + bs       # T-1 x K x H\n",
    "        #E_hs = torch.matmul(h[:-1].contiguous().view(T-1, 1, 1, H), As).squeeze(2) + bs       # T-1 x K x H\n",
    "        E_hs = torch.cat((bs.view(1, K, H).repeat(self.nlags,1,1) , E_hs), 0)                             # T x K x H\n",
    "        \n",
    "       # E_hs = torch.matmul(h[:-1].contiguous().view(T-1, 1, 1, H), As).squeeze(2) + bs       # T-1 x K x H\n",
    "       # E_hs = torch.cat((bs.view(1, K, H), E_hs), 0)                             # T x K x H\n",
    "        #lls = -0.5 * torch.sum((h.unsqueeze(1) - E_hs)**2 / Qs, dim=2)  # T x K\n",
    "        #lls += -0.5 * torch.sum(math.log(2 * math.pi) + torch.log(Qs), dim=1)            # K \n",
    "        #lls = -0.5 * torch.sum((h.unsqueeze(1) - E_hs)**2 / Qs, dim=2)  # T x K\n",
    "        #lls += -0.5 * torch.sum(math.log(2 * math.pi) + torch.log(Qs), dim=1)            # K \n",
    "        \n",
    "        lls = torch.zeros(T,K)\n",
    "        for i_t in range(T):\n",
    "            for i_k in range(K):\n",
    "                normal_dist = torch.distributions.multivariate_normal.MultivariateNormal(E_hs[i_t,i_k],covariance_matrix=Qs[i_k])\n",
    "                lls[i_t,i_k] = normal_dist.log_prob(h[i_t])\n",
    "                \n",
    "        # Forwards pass\n",
    "        alpha = torch.zeros_like(lls)\n",
    "        alpha[0] = -math.log(self.n_discrete_states) + lls[0] \n",
    "        for t in range(alpha.shape[0]-1):\n",
    "            alpha[t+1] = log_sum_exp(alpha[t] + log_PsT[t],dim=1) + lls[t+1]\n",
    "\n",
    "        # Backwards pass\n",
    "        beta = torch.zeros_like(lls)\n",
    "        for t in range(beta.shape[0]-2,-1,-1):\n",
    "            beta[t] = log_sum_exp(log_Ps[t]+beta[t+1]+lls[t+1],dim=1)\n",
    "\n",
    "        # Combine to get posterior over z\n",
    "        expected_z = alpha+beta\n",
    "        expected_z -= expected_z.max(1)[0].view((-1,1))\n",
    "        expected_z = torch.exp(expected_z)\n",
    "        expected_z /= expected_z.sum(1).view((-1,1))\n",
    "\n",
    "        return expected_z\n",
    " \n",
    "    def initialize_transitions(self, data_gen, nb_tng_batches, L2_reg=0.01):\n",
    "\n",
    "        As = torch.zeros((self.n_discrete_states, self.latent_dim_size_h*self.nlags, self.latent_dim_size_h))\n",
    "        bs = torch.zeros((self.n_discrete_states, self.latent_dim_size_h))\n",
    "        Qs = torch.zeros((self.n_discrete_states, self.latent_dim_size_h))\n",
    "\n",
    "        data_split = np.floor(nb_tng_batches/self.n_discrete_states)\n",
    "        \n",
    "        # Initialize\n",
    "        i_discrete_state = 0\n",
    "        XTX = Variable(torch.zeros((self.latent_dim_size_h*self.nlags+1,self.latent_dim_size_h*self.nlags+1)))\n",
    "        XTY = Variable(torch.zeros((self.latent_dim_size_h*self.nlags+1,self.latent_dim_size_h)))\n",
    "        \n",
    "        start_collecting=1\n",
    "\n",
    "        for batch_nb in range(nb_tng_batches):\n",
    "\n",
    "            pca_score, i_mouse, i_batch = next(data_gen)\n",
    "            pca_score = torch.autograd.Variable(torch.from_numpy(pca_score)).float()\n",
    "            X = torch.cat(([pca_score[nlags-1-i:pca_score.shape[0]-1-i] for i in range(nlags,-1,-1)]),dim=1) #pca_score[0:-1]\n",
    "            X = F.pad(X,(1,0),value=1)\n",
    "            Y = pca_score[self.nlags:]\n",
    "\n",
    "            XTX += torch.matmul(X.transpose(1,0),X)\n",
    "            XTY += torch.matmul(X.transpose(1,0),Y)\n",
    "\n",
    "            if start_collecting:\n",
    "                all_X = X\n",
    "                all_Y = Y\n",
    "                start_collecting=0\n",
    "            else:\n",
    "                all_X = torch.cat((all_X,X),0)\n",
    "                all_Y = torch.cat((all_Y,Y),0)\n",
    "            if i_discrete_state < self.n_discrete_states:\n",
    "                if np.mod(batch_nb+1,data_split)==0:\n",
    "                    reg_matrix = L2_reg*Variable(torch.eye(X.shape[1]))\n",
    "                    reg_XTX = XTX+reg_matrix\n",
    "                    XTX_inv = torch.inverse(reg_XTX)\n",
    "                    W = torch.matmul(XTX_inv,XTY)\n",
    "\n",
    "                    As[i_discrete_state] = W[1:,:].data\n",
    "                    bs[i_discrete_state] = W[0,:].data\n",
    "\n",
    "                    # Reconstruct to get residuals \n",
    "                    Y_hat = torch.matmul(all_X,W)\n",
    "                    residuals = Y_hat-all_Y\n",
    "                    Qs[i_discrete_state] = torch.var(residuals,0).data\n",
    "\n",
    "                    start_collecting=1\n",
    "\n",
    "                    i_discrete_state +=1\n",
    "                    XTX = Variable(torch.zeros((self.latent_dim_size_h*self.nlags+1,self.latent_dim_size_h*self.nlags+1)))\n",
    "                    XTY = Variable(torch.zeros((self.latent_dim_size_h*self.nlags+1,self.latent_dim_size_h)))\n",
    "        if i_discrete_state < self.n_discrete_states-1:\n",
    "            print('ERROR WITH INITIALIZATION')\n",
    "            sys.exit(0)\n",
    "\n",
    "        inv_softplus_Qs = torch.log(torch.exp(Qs)-1)\n",
    "        return As, bs, inv_softplus_Qs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kappa=0.9\n",
    "K=100\n",
    "H=10\n",
    "nlags=3\n",
    "model = ARHMM(K,H,kappa,batch_size,nlags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initialize model\n",
    "As, bs, inv_softplus_Qs = model.initialize_transitions(data_gen, nb_tng_batches)\n",
    "model.As.data = As\n",
    "model.bs.data = bs\n",
    "model.inv_softplus_Qs.data = inv_softplus_Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at initialization autoregressive forward pass results\n",
    "# T = 230\n",
    "# i_z = 20\n",
    "# h = np.zeros((T, 10))\n",
    "# h[0] = bs[i_z].numpy()\n",
    "# Qs = np.log(np.exp(inv_softplus_Qs.numpy())+1)\n",
    "# for t in range(T-1):\n",
    "#     h[t+1] = np.dot(As[i_z].numpy().T,h[t])+bs[i_z].numpy() + Qs[i_z]**(1/2)*np.random.normal(size=(10,))\n",
    "\n",
    "# plt.plot(h);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#log_Ps, As, bs, Qs = model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### Analysis of their model ########\n",
    "\n",
    "# ar_mat = np.load('ar_mat.npy')\n",
    "# sig = np.load('sig.npy')\n",
    "# transition_matrix = np.load('transition_matrix.npy')\n",
    "import joblib\n",
    "\n",
    "model_fit = 'clean_data_pca_gibbs_sampling_100_iter.p'\n",
    "model_fit = joblib.load(model_fit)\n",
    "\n",
    "kappa=0.9\n",
    "K=100\n",
    "H=10\n",
    "nlags=3\n",
    "\n",
    "\n",
    "model2 = ARHMM(K,H,kappa,batch_size,nlags)\n",
    "\n",
    "\n",
    "model2.As.data = torch.tensor(np.swapaxes(np.asarray(model_fit['model_parameters'][0]['ar_mat'])[:,:,:-1],2,1)).float()\n",
    "model2.bs.data = torch.tensor(np.asarray(model_fit['model_parameters'][0]['ar_mat'])[:,:,-1]).float()\n",
    "sig = np.asarray(model_fit['model_parameters'][0]['sig'])\n",
    "diag_sig = np.asarray([np.diag(sig[i]) for i in range(100)])\n",
    "Qs = torch.tensor(sig).float()\n",
    "model2.inv_softplus_Qs.data = torch.tensor(np.log(np.exp(diag_sig)-1)).float()\n",
    "model2.log_transition_proba.data = torch.tensor(np.log(np.asarray(model_fit['model_parameters'][0]['transition_matrix']))).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 901/901 [07:09<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0   2705177088.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-2)\n",
    "for i_epoch in range(1):\n",
    "    train_loss=0\n",
    "    for i_train in tqdm(range(nb_tng_batches)):\n",
    "        pca, i_mouse, i_batch = next(data_gen)\n",
    "        pca = torch.tensor(pca).to(device).float()\n",
    "        #pca_torch = torch.autograd.Variable(torch.from_numpy(pca)).float()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model2.loss(pca,*model2())\n",
    "        if i_epoch > 0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss+=loss.item()\n",
    "    print(\"Epoch: \", i_epoch, \" \", train_loss/nb_tng_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 901/901 [07:12<00:00,  2.08it/s]\n",
      "  0%|          | 0/901 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0   683965312.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 14/901 [00:56<59:44,  4.04s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-be912cbb0caa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=.01)\n",
    "for i_epoch in range(100):\n",
    "    train_loss=0\n",
    "    for i_train in tqdm(range(nb_tng_batches)):\n",
    "        pca, i_mouse, i_batch = next(data_gen)\n",
    "        pca = torch.tensor(pca).to(device).float()\n",
    "        #pca_torch = torch.autograd.Variable(torch.from_numpy(pca)).float()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pca,*model())\n",
    "        if i_epoch > 0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss+=loss.item()\n",
    "    print(\"Epoch: \", i_epoch, \" \", train_loss/nb_tng_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEW! Train with the EM loss\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=.01)\n",
    "for i_epoch in range(100):\n",
    "    train_loss = 0\n",
    "    for i_train in tqdm(range(nb_tng_batches)):\n",
    "        pca, i_mouse, i_batch = next(data_gen)\n",
    "        pca = torch.tensor(pca).to(device).float()\n",
    "        #pca_torch = torch.autograd.Variable(torch.from_numpy(pca)).float()\n",
    "        \n",
    "        zs = model.sample_states(pca)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model.EM_loss(zs, pca, *model())\n",
    "        if i_epoch > 0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(\"Epoch: \", i_epoch, \" \", train_loss/nb_tng_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.exp(model.log_transition_proba.detach().numpy()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(model.As[4].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### Analysis of their model ########\n",
    "\n",
    "# ar_mat = np.load('ar_mat.npy')\n",
    "# sig = np.load('sig.npy')\n",
    "# transition_matrix = np.load('transition_matrix.npy')\n",
    "import joblib\n",
    "\n",
    "model_fit = 'clean_data_pca_gibbs_sampling_100_iter.p'\n",
    "model_fit = joblib.load(model_fit)\n",
    "\n",
    "kappa=0.9\n",
    "K=100\n",
    "H=10\n",
    "nlags=3\n",
    "\n",
    "\n",
    "model = ARHMM(K,H,kappa,batch_size,nlags)\n",
    "\n",
    "\n",
    "model.As.data = torch.tensor(np.swapaxes(np.asarray(model_fit['model_parameters'][0]['ar_mat'])[:,:,:-1],2,1)).float()\n",
    "model.bs.data = torch.tensor(np.asarray(model_fit['model_parameters'][0]['ar_mat'])[:,:,-1]).float()\n",
    "sig = np.asarray(model_fit['model_parameters'][0]['sig'])\n",
    "diag_sig = np.asarray([np.diag(sig[i]) for i in range(100)])\n",
    "Qs = torch.tensor(sig).float()\n",
    "model.inv_softplus_Qs.data = torch.tensor(np.log(np.exp(diag_sig)-1)).float()\n",
    "# np.log(np.asarray(model_fit['model_parameters'][0]['transition_matrix']\n",
    "model.log_transition_proba.data = torch.tensor(np.log(np.asarray(model_fit['model_parameters'][0]['transition_matrix']))).float()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_matrix = torch.tensor(np.asarray(model_fit['model_parameters'][0]['transition_matrix'])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(transition_matrix.detach().numpy())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_matrix[20,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(transition_matrix[10].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha=6\n",
    "kappa=1e8\n",
    "i_state=1\n",
    "concentration = alpha*torch.ones(K)\n",
    "concentration[i_state] += kappa\n",
    "dirichlet_dist = torch.distributions.dirichlet.Dirichlet(concentration)\n",
    "#dirichlet_dist = torch.distributions.dirichlet.Dirichlet(transition_matrix[i_state])\n",
    "dirichlet_dist.log_prob(transition_matrix[i_state]) #/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "scipy.stats.dirichlet.logpdf(transition_matrix[i_state].detach().numpy(),concentration.detach().numpy()) #/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.dirichlet.logpdf(transition_matrix[i_state].detach().numpy(),transition_matrix[i_state].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.exp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K=100\n",
    "for i_state in range(K):\n",
    "    concentration = alpha*torch.ones(K)\n",
    "    concentration[i_state] += kappa\n",
    "    dirichlet_dist = torch.distributions.dirichlet.Dirichlet(concentration)\n",
    "    if i_state==0:\n",
    "        prior_trans = torch.sum(dirichlet_dist.log_prob(Ps[i_state]))\n",
    "    else:\n",
    "        prior_trans += torch.sum(dirichlet_dist.log_prob(Ps[i_state]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha=6\n",
    "kappa=1e8\n",
    "K=100\n",
    "i_state = 1\n",
    "\n",
    "concentration = alpha*torch.ones(K)\n",
    "concentration[i_state] += kappa\n",
    "dirichlet_dist = torch.distributions.dirichlet.Dirichlet(concentration)\n",
    "-dirichlet_dist.log_prob(transition_matrix[i_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pca, i_mouse, which_batch = next(data_gen)\n",
    "pca = torch.tensor(pca).to(device).float()\n",
    "model2 = ARHMM(K,H,kappa,batch_size,nlags)\n",
    "\n",
    "loss = model.loss(pca,*model())\n",
    "loss2 = model2.loss(pca,*model2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca, i_mouse, which_batch = next(data_gen)\n",
    "pca = torch.tensor(pca).to(device).float()\n",
    "expected_z = model.get_expected_states(pca,*model())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(expected_z.data.numpy().T[:,0:300],origin='lower',cmap='gray_r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca, i_mouse, which_batch = next(data_gen)\n",
    "pca = torch.tensor(pca).to(device).float()\n",
    "log_Ps, As, bs, not_Qs = model()\n",
    "\n",
    "expected_z = model.get_expected_states_covmat(pca,log_Ps, As, bs,Qs)\n",
    "real_labels = model_fit['labels'][0][model_fit['keys'].index(uuids[i_mouse])].reshape((-1,))[which_batch*batch_size:(which_batch+1)*batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(expected_z.data.numpy().T[:,0:300],origin='lower',cmap='gray_r')\n",
    "#plt.plot(np.argmax(expected_z.data.numpy(),1)[0:500],'r')\n",
    "plt.plot(real_labels[:300],'r',alpha=.2)\n",
    "plt.ylim([0,105])\n",
    "plt.colorbar()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Discrete state')\n",
    "#plt.savefig('gibbs_states.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(real_labels)\n",
    "#plt.plot(np.argmax(expected_z.data.numpy(),1),'r',alpha=.5)\n",
    "plt.xlim([0,300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.argmax(expected_z.data.numpy(),1))\n",
    "plt.xlim([0,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow((model.log_transition_proba+Variable(6 * torch.eye(model.log_transition_proba.shape[0]))).data.numpy())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.exp((model.log_transition_proba+Variable(6 * torch.eye(model.log_transition_proba.shape[0]))).data.numpy()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rr = np.load('keys.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datta4",
   "language": "python",
   "name": "datta4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
